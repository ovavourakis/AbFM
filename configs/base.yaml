data:
  module:
    inference_only_mode: False
  dataset:
    seed: 123
    pdbs: # config for known structures
      csv_path: ./preprocessed/metadata.csv # metadata and paths to preprocessed data per antibody
      min_num_res: 215
      max_num_res: 260
      subset: null # take 1st k instances from dataset that fit the seq-length criteria (null = all)
    train:
      use_pdbs: True # TODO: these aren't being used
      pdbs_per_val: None # all pdbs
      generate: False
    valid:
      use_pdbs: True
      pdbs_per_val: 100 
      generate: True
      samples:
        seed: 123   # determines random length combos per tot_length
        min_length: ${data.dataset.pdbs.min_num_res}
        max_length: ${data.dataset.pdbs.min_num_res}
        min_length_heavy: 109
        max_length_heavy: 145
        min_length_light: 99
        max_length_light: 121

        samples_per_length: 1
        length_step: 1
        length_list: null
        overwrite: False
    test:
      use_pdbs: True
      pdbs_per_val: None # all pdbs
      generate: True
      samples:
        min_length: ${data.dataset.pdbs.min_num_res}
        max_length: ${data.dataset.pdbs.min_num_res}
        min_length_heavy: 109
        max_length_heavy: 145
        min_length_light: 99
        max_length_light: 121

        samples_per_length: 5
        length_step: 1
        length_list: null
        overwrite: False

    loaders:
      num_workers: 4
      prefetch_factor: 10

    gens: # config for structures to be generated (during val/test)
      samples_per_gen_length: 5   # sample k seqs (WITH replacement) per seq-length to eval on
      num_gen_lengths: 8          # num different seq-lengths to generate at during validation
      max_gen_length: 260         # max seq-length to generate at during validation
    
    # TODO: add config for how many real structures to validate on?
    # TODO: have separate congis for valid and test (more samples for test)
  
  sampler: # config for batch sampler (LengthBatcher)
    # rename to bsampler
    # should have one of these for each of train, valid, test, sample
    num_batches_per_epoch_per_gpu: 2000 # more than strucs/2/100; same as in FrameFlow
    num_struc_samples: null
    max_batch_size: 100
    max_num_res_squared: 500_000
    # NOTE: actual_batch_size = min(max_batch_size, max_num_res_squared // seq_len^2 + 1)





interpolant:
  min_t: 1e-2
  rots:
    train_schedule: linear  # TODO: setting to 'exp' will have no effect? rot_sample_kappa not being called 
    sample_schedule: exp 
    exp_rate: 10
  trans:
    train_schedule: linear  # TODO: setting to 'exp' will have no effect? rot_sample_kappa not being called 
    sample_schedule: linear # TODO: setting to 'exp' will have no effect; alternative euler step for translations not implemented -> interpolant._trans_euler_step
  sampling:
    num_timesteps: 100
  self_condition: ${model.edge_features.self_condition}

model:
  node_embed_size: 256
  edge_embed_size: 128
  symmetric: False
  node_features:
    c_s: ${model.node_embed_size}
    c_pos_emb: 128
    c_timestep_emb: 128
    embed_diffuse_mask: False
    max_num_res: 2000
    timestep_int: 1000
  edge_features:
    single_bias_transition_n: 2
    c_s: ${model.node_embed_size}
    c_p: ${model.edge_embed_size}
    relpos_k: 64
    use_rbf: True
    num_rbf: 32
    feat_dim: 64
    num_bins: 22
    self_condition: True # edge reps include distogram of previous-step t1-calpha-location-prediction (applied stochastically, 50% of the time -> FlowModule.trainig_step())
  ipa:
    c_s: ${model.node_embed_size}
    c_z: ${model.edge_embed_size}
    c_hidden: 128
    no_heads: 8
    no_qk_points: 8
    no_v_points: 12
    seq_tfmr_num_heads: 4
    seq_tfmr_num_layers: 2
    num_blocks: 6

experiment:
  debug: False
  seed: 123
  num_devices: 2
  warm_start: null                # path to override-config for warm-starting
  warm_start_cfg_override: True
  use_swa: False                  # TODO: deprecated?
  batch_ot:                       # TODO: entire section deprecated, as far as I can see
    enabled: True
    cost: kabsch
    noise_per_sample: 1
    permute: False
  training:
    min_plddt_mask: null
    loss: se3_vf_loss # in {"bb_atom_loss", "trans_loss", "dist_mat_loss", "auxiliary_loss", "rots_vf_loss", "se3_vf_loss"}
                      # will train on the loss specified + 'auxiliary_loss' (FlowModule->training_step())
    bb_atom_scale: 0.1            # convert coordinates from Angstrom to nm
    trans_scale: 0.1              # convert coordinates from Angstrom to nm
    translation_loss_weight: 2.0
    t_normalize_clip: 0.9         # clip loss normaliser 1/(1-t) to [1/(0.1), 1/1]
    rotation_loss_weights: 1.0
    aux_loss_weight: 1.0
    aux_loss_t_pass: 0.25         # use auxiliary losses from t in [0,k]; k<1
  wandb:
    name: baseline
    project: se3-fm
    save_code: True
    tags: []
  optimizer:
    lr: 0.0001
  trainer:
    overfit_batches: 0            # set to non-zero to sanity-check model capacity
    min_epochs: 1                 # prevents early stopping
    max_epochs: 200
    accelerator: gpu
    log_every_n_steps: 1
    deterministic: False          # allow PyTorch to use non-deterministic algorithms
    strategy: ddp
    check_val_every_n_epoch: 2
    accumulate_grad_batches: 2  # accumulate gradient over k * num_GPUs batches
  checkpointer:
    dirpath: ckpt/${experiment.wandb.project}/${experiment.wandb.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
    save_last: True
    save_top_k: 3
    monitor: valid/non_coil_percent # TODO: we want to monitor AB like-ness
    mode: max
