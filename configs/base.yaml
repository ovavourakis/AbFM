data:
  module:                                   # config for data module
    inference_only_mode: False
    loaders:                                # config for individual data loader processes
        num_workers: 2                      # per dataloader (one per GPU, train, val and test) - should provide appropriate #CPUs to match
        prefetch_factor: 10
  dataset:
    pdbs:
      csv_path: ./ab_processed_newclust/metadata.csv # metadata and paths to preprocessed data per antibody
      min_num_res: 215
      max_num_res: 260
      subset: null                          # take 1st k instances from dataset that fit the seq-length criteria (null = all)
    train:
      bsampler:                             # config for batch sampler (of pdb structures)
        batch_size: 10                      # choose small, so that loose few structures (and total lengths) through incomplete-batch omission each epoch
        num_struc_samples: null             # null: train on all structures in train set; else subsample top k (before splitting across replicas)
    valid:
      use_pdbs: True                        # run validation on known structures
      generate: False                       # also validate using some de-novo generations (currently undefined how to do that)
      samples:
        seed: 123                           # determines random length combos chosen for tot_length
        min_length: ${data.dataset.pdbs.min_num_res}
        max_length: ${data.dataset.pdbs.max_num_res}
        min_length_heavy: 109
        max_length_heavy: 145
        min_length_light: 99
        max_length_light: 121
        samples_per_length: 1
        length_step: 5
        length_list: null                   # explicit list of lengths at which to generate
        overwrite: False
      bsampler:                             # config for batch sampler (only applies to known pdb structrues, not generations)
        batch_size: 10                      # choose small, so that loose few structures through incomplete-batch omission
        num_struc_samples: 1000
    test:
      use_pdbs: True                        # run test on known structures
      generate: False                       # also generate some de-novo structures and run tests on them (currently undefined how to do that)
      samples:
        seed: 123                           # determines random length combos chosen for tot_length
        min_length: ${data.dataset.pdbs.min_num_res}
        max_length: ${data.dataset.pdbs.max_num_res}
        min_length_heavy: ${data.dataset.valid.samples.min_length_heavy}
        max_length_heavy: ${data.dataset.valid.samples.max_length_heavy}
        min_length_light: ${data.dataset.valid.samples.min_length_light}
        max_length_light: ${data.dataset.valid.samples.max_length_light}
        samples_per_length: 4
        length_step: 1
        length_list: null                   # explicit list of lengths at which to generate
        overwrite: False
      bsampler:                             # config for batch sampler (only applies to known pdb structrues, not generations)
        batch_size: 10                      # choose small, so that loose few structures through incomplete-batch omission
        num_struc_samples: null

interpolant:
  min_t: 1e-2
  self_condition: ${model.edge_features.self_condition}
  rots:
    train_schedule: linear                  # 'linear' or 'exp' (NOTE: however, differential by-component loss normalisation for rots and trans with different t not implemented -> FlowModule.model_step())
    sample_schedule: exp                    # 'linear' or 'exp'
    exp_rate: 10
  trans:
    train_schedule: linear                  # 'linear' or 'exp' (NOTE: however, differential by-component loss normalisation for rots and trans with different t not implemented -> FlowModule.model_step())
    sample_schedule: linear                 # TODO: only 'linear' implemented; non-linear euler step for translations not implemented -> Interpolant._trans_euler_step()
    exp_rate: 10
  sampling:
    num_timesteps: 100

model:
  node_embed_size: 256
  edge_embed_size: 128
  node_features:
    c_s: ${model.node_embed_size}
    c_pos_emb: 128
    c_timestep_emb: 128
  edge_features:
    c_s: ${model.node_embed_size}
    c_p: ${model.edge_embed_size}
    feat_dim: 64
    num_bins: 22
    self_condition: True                    # edge reps include distogram of previous-step t1-calpha-location-prediction (applied stochastically, 50% of the time -> FlowModule.process_struc_batch())
  ipa:
    c_s: ${model.node_embed_size}
    c_z: ${model.edge_embed_size}
    c_hidden: 128
    no_heads: 8
    no_qk_points: 8
    no_v_points: 12
    seq_tfmr_num_heads: 4
    seq_tfmr_num_layers: 2
    num_blocks: 6

experiment:                       # config for ModelRun
  debug: False
  num_devices: 2                  # max number of GPUs to use, if available
  warm_start_cfg_override: True   # if True, will override this config with the one from the checkpoint
  warm_start: null                # path to directory that contains *config* of checkpoint to warm-start from
  warm_start_weights: weights/published.ckpt # fine-tune starting from these weigths
  crash_dir: /vols/opig/users/vavourakis/crash_dir_latest                 # path to save batch info upon encountering NaN loss
  training:
    loss: tot_loss                # in {"tot_loss", "bb_atom_loss", "trans_loss", "dist_mat_loss", "auxiliary_loss", "rots_vf_loss", "se3_vf_loss"}
    translation_loss_weight: 2.0
    rotation_loss_weights: 0.3
    aux_loss_weight: 1.0          # in relation to rototrans-losses considered together
    aux_loss_t_pass: 0.75          # use auxiliary losses from t in [k,1]; 0<k<1
    t_normalize_clip: 0.9         # clip loss normaliser 1/(1-min(t,0.9))
    bb_atom_scale: 0.1            # convert coordinates from Angstrom to nm
    trans_scale: 0.1              # convert coordinates from Angstrom to nm
    min_plddt_mask: null          # NOTE: filter implemented, but our pre-processing does not read it out of our structures, so would have to re-work that first
  validation:
    loss: ${experiment.training.loss}
    translation_loss_weight: ${experiment.training.translation_loss_weight}
    rotation_loss_weights: ${experiment.training.rotation_loss_weights}
    aux_loss_weight: ${experiment.training.aux_loss_weight}
    aux_loss_t_pass: ${experiment.training.aux_loss_t_pass}
    t_normalize_clip: ${experiment.training.t_normalize_clip}
    bb_atom_scale: ${experiment.training.bb_atom_scale}
    trans_scale: ${experiment.training.trans_scale}
    min_plddt_mask: ${experiment.training.min_plddt_mask}
  testing:
    ckpt_name : last.ckpt         # located in checkpointer.dirpath
    loss: ${experiment.training.loss}
    translation_loss_weight: ${experiment.training.translation_loss_weight}
    rotation_loss_weights: ${experiment.training.rotation_loss_weights}
    aux_loss_weight: ${experiment.training.aux_loss_weight}
    aux_loss_t_pass: ${experiment.training.aux_loss_t_pass}
    t_normalize_clip: ${experiment.training.t_normalize_clip}
    bb_atom_scale: ${experiment.training.bb_atom_scale}
    trans_scale: ${experiment.training.trans_scale}
    min_plddt_mask: ${experiment.training.min_plddt_mask}
  optimizer:
    lr: 1e-4
  wandb:
    name: VH_only
    project: abfm
    save_code: True
    tags: []
  trainer:
    overfit_batches: 0            # set to non-zero to sanity-check model capacity
    min_epochs: 1                 # prevents early stopping
    max_epochs: 220 #70 #200
    accelerator: gpu
    log_every_n_steps: 1
    deterministic: False          # allow PyTorch to use non-deterministic algorithms
    strategy: ddp
    check_val_every_n_epoch: 1
    # val_check_interval: 1000      # validate every k batches of size batch_size (pre-gradient cumulation != gradient steps = global_step) 
    accumulate_grad_batches: 6    # accumulate gradient over k batches (implemented independent of #GPUs)
  checkpointer:
    dirpath: ckpt/${experiment.wandb.project}/${experiment.wandb.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
    save_last: True
    save_top_k: 5
    monitor: valid/loss                                                                                                         # TODO: we want to monitor AB like-ness of *generations*, too
    mode: min
    every_n_epochs: 1
  lr_scheduler:                   # config for learning ReduceLROnPlateau scheduler
    factor: 0.2
    patience: 3
    monitor: ${experiment.checkpointer.monitor}
    interval: epoch
    frequency: ${experiment.trainer.check_val_every_n_epoch}
    strict: False
