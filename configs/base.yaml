data:
  module:                                   # config for data module
    inference_only_mode: False
    loaders:
        num_workers: 4
        prefetch_factor: 10
  dataset:
    pdbs:
      csv_path: ./ab_processed/metadata.csv # metadata and paths to preprocessed data per antibody
      min_num_res: 215
      max_num_res: 260
      subset: null                          # take 1st k instances from dataset that fit the seq-length criteria (null = all)
    train:
      bsampler:                             # config for batch sampler
        seed: 123
        num_batches_per_epoch_per_gpu: 2 #2000 # more than train_strucs/num_gpus/max_batch_size; same value as in FrameFlow
        num_struc_samples: null             # train on all structures in train set
        max_batch_size: 100
        max_num_res_squared: 500_000        # true_batch_size = min(max_batch_size, max_num_res_squared // seq_len^2 + 1)   
    valid:
      use_pdbs: True                        # run validation on known structures
      generate: True                        # also validate some de-novo generations (currently undefined how to do that)
      samples:
        seed: 123                           # determines random length combos chosen for tot_length
        min_length: 255 #${data.dataset.pdbs.min_num_res}
        max_length: ${data.dataset.pdbs.max_num_res}
        min_length_heavy: 109
        max_length_heavy: 145
        min_length_light: 99
        max_length_light: 121
        samples_per_length: 1
        length_step: 1
        length_list: null                   # explicit list of lengths at which to generate
        overwrite: False
      bsampler:                             # config for batch sampler
        seed: 123
        num_batches_per_epoch_per_gpu: 1    # set close to num_struc_samples/num_gpus/ (and allow enough for all samples)
        num_struc_samples: 100
        max_batch_size: 50
        max_num_res_squared: 500_000        # true_batch_size = min(max_batch_size, max_num_res_squared // seq_len^2 + 1)   
    test:
      use_pdbs: True                        # run test on known structures
      generate: True                       # also generate some de-novo structures and run tests on them (currently undefined how to do that)
      samples:
        seed: 123                           # determines random length combos chosen for tot_length
        min_length: 255 #${data.dataset.pdbs.min_num_res}
        max_length: ${data.dataset.pdbs.max_num_res}
        min_length_heavy: ${data.dataset.valid.samples.min_length_heavy}
        max_length_heavy: ${data.dataset.valid.samples.max_length_heavy}
        min_length_light: ${data.dataset.valid.samples.min_length_light}
        max_length_light: ${data.dataset.valid.samples.max_length_light}
        samples_per_length: 1
        length_step: 1
        length_list: null                   # explicit list of lengths at which to generate
        overwrite: False
      bsampler:                             # config for batch sampler
        seed: 123
        num_batches_per_epoch_per_gpu: 75   # set close to test_strucs/num_gpu/max_batch_size
        num_struc_samples: null
        max_batch_size: 100
        max_num_res_squared: 500_000        # true_batch_size = min(max_batch_size, max_num_res_squared // seq_len^2 + 1)

interpolant:
  min_t: 1e-2
  self_condition: ${model.edge_features.self_condition}
  rots:
    train_schedule: linear                  # 'linear' or 'exp' (NOTE: however, differential by-component loss normalisation for rots and trans with different t not implemented -> FlowModule.model_step())
    sample_schedule: exp                    # 'linear' or 'exp'
    exp_rate: 10
  trans:
    train_schedule: linear                  # 'linear' or 'exp' (NOTE: however, differential by-component loss normalisation for rots and trans with different t not implemented -> FlowModule.model_step())
    sample_schedule: linear                 # TODO: only 'linear' implemented; non-linear euler step for translations not implemented -> Interpolant._trans_euler_step()
  sampling:
    num_timesteps: 100

model:
  node_embed_size: 256
  edge_embed_size: 128
  node_features:
    c_s: ${model.node_embed_size}
    c_pos_emb: 128
    c_timestep_emb: 128
  edge_features:
    c_s: ${model.node_embed_size}
    c_p: ${model.edge_embed_size}
    feat_dim: 64
    num_bins: 22
    self_condition: True                    # edge reps include distogram of previous-step t1-calpha-location-prediction (applied stochastically, 50% of the time -> FlowModule.process_struc_batch())
  ipa:
    c_s: ${model.node_embed_size}
    c_z: ${model.edge_embed_size}
    c_hidden: 128
    no_heads: 8
    no_qk_points: 8
    no_v_points: 12
    seq_tfmr_num_heads: 4
    seq_tfmr_num_layers: 2
    num_blocks: 6

experiment:                       # config for ModelRun
  debug: False
  num_devices: 2
  warm_start_cfg_override: True   # if True, will override this config with the one from the checkpoint
  warm_start: null                # path to directory that contains config of checkpoint to warm-start from
  training:
    loss: tot_loss                # in {"tot_loss", "bb_atom_loss", "trans_loss", "dist_mat_loss", "auxiliary_loss", "rots_vf_loss", "se3_vf_loss"}
                                  # ! WARNING ! : will return the loss specified + 'auxiliary_loss' (FlowModule->loss_agg_and_log())
    translation_loss_weight: 2.0
    rotation_loss_weights: 1.0
    aux_loss_weight: 1.0          # in relation to rototrans-losses considered together
    aux_loss_t_pass: 0.25         # use auxiliary losses from t in [0,k]; k<1
    t_normalize_clip: 0.9         # clip loss normaliser 1/(1-t) to [1/(0.1), 1/1]
    bb_atom_scale: 0.1            # convert coordinates from Angstrom to nm
    trans_scale: 0.1              # convert coordinates from Angstrom to nm
    min_plddt_mask: null          # NOTE: filter implemented, but our pre-processing does not read it out of our structures, so would have to re-work that first
  validation:
    loss: tot_loss                # in {"tot_loss", "bb_atom_loss", "trans_loss", "dist_mat_loss", "auxiliary_loss", "rots_vf_loss", "se3_vf_loss"}
                                  # ! WARNING ! : will return the loss specified + 'auxiliary_loss' (FlowModule->loss_agg_and_log())
    translation_loss_weight: 2.0
    rotation_loss_weights: 1.0
    aux_loss_weight: 1.0          # in relation to rototrans-losses considered together
    aux_loss_t_pass: 0.25         # use auxiliary losses from t in [0,k]; k<1
    t_normalize_clip: 0.9         # clip loss normaliser 1/(1-t) to [1/(0.1), 1/1]
    bb_atom_scale: 0.1            # convert coordinates from Angstrom to nm
    trans_scale: 0.1              # convert coordinates from Angstrom to nm
    min_plddt_mask: null          # NOTE: filter implemented, but our pre-processing does not read it out of our structures, so would have to re-work that first
  testing:
    ckpt_name : last.ckpt         # located in checkpointer.dirpath
    loss: tot_loss                # in {"tot_loss", "bb_atom_loss", "trans_loss", "dist_mat_loss", "auxiliary_loss", "rots_vf_loss", "se3_vf_loss"}
                                  # ! WARNING ! : will return the loss specified + 'auxiliary_loss' (FlowModule->loss_agg_and_log())
    translation_loss_weight: 2.0
    rotation_loss_weights: 1.0
    aux_loss_weight: 1.0          # in relation to rototrans-losses considered together
    aux_loss_t_pass: 0.25         # use auxiliary losses from t in [0,k]; k<1
    t_normalize_clip: 0.9         # clip loss normaliser 1/(1-t) to [1/(0.1), 1/1]
    bb_atom_scale: 0.1            # convert coordinates from Angstrom to nm
    trans_scale: 0.1              # convert coordinates from Angstrom to nm
    min_plddt_mask: null          # NOTE: filter implemented, but our pre-processing does not read it out of our structures, so would have to re-work that first
  optimizer:
    lr: 0.0001
  wandb:
    name: train_debug
    project: abfm
    save_code: True
    tags: []
  trainer:
    overfit_batches: 0            # set to non-zero to sanity-check model capacity
    min_epochs: 1                 # prevents early stopping
    max_epochs: 4 #200
    accelerator: gpu
    log_every_n_steps: 1
    deterministic: False          # allow PyTorch to use non-deterministic algorithms
    strategy: ddp
    check_val_every_n_epoch: 2
    accumulate_grad_batches: 2    # accumulate gradient over k * num_GPUs batches
  checkpointer:
    dirpath: ckpt/${experiment.wandb.project}/${experiment.wandb.name}/2024-04-16_15-43-44   #${now:%Y-%m-%d}_${now:%H-%M-%S}
    save_last: True
    save_top_k: 3
    monitor: valid/loss       # TODO: we want to monitor AB like-ness of *generations*, too
    mode: min
